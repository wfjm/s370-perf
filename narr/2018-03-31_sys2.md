## Narrative: 2018-03-31_sys2

**Objective**: generate **Reference data** with feature complete s370_perf
on the current `Intel Xeon E5-1620` reference system.
Will be used as base line for the comparison of other systems.

| Attribute | Value |
| --------- | ----- |
| Host   | [sys2](hostinfo_sys2.md) |
| System | Herc tk4- 08 (NUMCPU=2 MAXCPU=2) |
| s370_perf | [V0.9.7  rev  1003  2018-03-30](https://github.com/wfjm/s370-perf/blob/2685ff0/codes/s370_perf.asm) |
| Creation Date | 2018-03-31 |
| Send by | wfjm |
| [cpufreq](README_narr.md#user-content-cpufreq) | governor: ondemand; latency: 10.0 us |
| [eff. CPU clock](README_narr.md#user-content-effclk) | 3600 MHz |
| [ASM step](README_narr.md#user-content-asm) | `CPU:    5.24s  20.75%; CPU/ela:  92.09%` |
| [GO step](README_narr.md#user-content-go)   | `CPU:  277.14s   1.19%; CPU/ela:  99.92%` |
| Data | [2018-03-31_sys2.dat](../data/2018-03-31_sys2.dat) |
|      | [2018-03-31_sys2-raw.dat](../data/2018-03-31_sys2-raw.dat) _(raw view)_ |
|      | [2018-03-31_sys2-ins.dat](../data/2018-03-31_sys2-ins.dat) _(by instruction view)_ |
| Summaries | [sys2 over time](sum_2018-03-31_sys2_and_old-runs.dat) - _absolute instruction time_ |
| [lmark](README_narr.md#user-content-lmark) | 115.88 MIPS |

### <a id="find">Findings</a>
- **General**
  - the test reproducibility is very good, the median of the 50% width is 0.89%.
  - the test results are stable over time, with three versions of s370_perf
    between 2018-01-06 and 2018-03-31, see
    [section sys2 over time](#user-content-find-over-time)
  - T9xx tests show _significant deviations_ from a simple additive
    instruction timing model.
    See section [additivity of instruction times](#user-content-find-itadd).
- **Instruction timing**
  - the simplest instruction, `LR R,R` is amazingly fast, only 2.09 ns,
    see section [LR timing](#user-content-find-lr).
  - `LTR` is with 3.52 ns about 1.7 times slower than `LR`,
    see section [LTR timing](#user-content-find-ltr).
  - `L` and `ST` are with 7.47 and 7.71 ns about 3.7 times slower than `LR`,
    see section [L+ST timing](#user-content-find-l).
  - unaligned memory access takes longer, sometimes, the pattern is surprising,
    see section [unaligned access](#user-content-find-unal).
  - branch to same page is faster than to different page,
    see section [branch timing](#user-content-find-bfar).
  - memory access speed doesn't depend of data location,
    see section [memory timing](#user-content-find-mfar).
  - `ALR` is faster than `AR`,
    see section [ALR timing](#user-content-find-alr).
  - astonishingly good [floating point performance](#user-content-find-float).
  - `MVC` is very efficient, except for byte propagation,
    see section [MVC performance](#user-content-find-mvc).
  - `MVCIN` is quite slow, a factor 6 slower than `MVN` or `MVZ`,
    see section [MVCIN performance](#user-content-find-mvcin).
  - `MVCL` has a slightly higher setup time as `MVC`,
    see section [MVCL performance](#user-content-find-mvcl).
  - `CLC` is a factor 5.3 slower than `MVC`, the performance in line with
    other byte-by-byte instructions, see
    section [CLC performance](#user-content-find-clc).
  - `CLCL` is a factor of 12 slower than `CLC`,
    see section [CLCL performance](#user-content-find-clcl).
  - `TRT` is a factor of 12 slower than `TR`,
    see section [TRT performance](#user-content-find-trt).
  - speed of decimal arithmetic seems independent of digit count,
    except for `DP`,
    see section [decimal performance](#user-content-find-dec).
  - `CS`,`CDS` and `TS` slow in the _lock missed_ case for multi-CPU setups,
    see section [CS, CDS, TS performance](#user-content-find-lock).
  - `STCK` quite fast,
    see section [STCK performance](#user-content-find-stck).
- **Instruction timing comparisons**
  - [nbk1 vs sys2](2018-04-01_nbk1.md#user-content-find-vs-sys2) -
    compare with `Intel Core i7-3520M` based system.
  - [nbk2 vs sys2](2018-04-01_nbk2.md#user-content-find-vs-sys2) -
    compare with `Intel Core i5 M 520` based system.
  - [sys1 vs sys2](2018-04-01_sys1-08.md#user-content-find-vs-sys2) -
    compare with `Intel Core2 Duo E8400` based system.
  - [sys2 vs srv4](2018-04-28_srv4.md#user-content-find-vs-sys2) -
    compare with `Intel Xeon E5-2680 v4` based system.
  - [rasp2b vs sys2](2018-04-02_rasp2b.md#user-content-find-vs-sys2) -
    compare with `ARMv7 RaspBerry 2B` based system.

#### <a id="find-over-time">sys2 tests stable over time</a>
The [sys2 over time](sum_2018-03-31_sys2_and_old-runs.dat) summary compares
the instruction timing measured with three versions of s370_perf between
2018-01-06 and 2018-03-31

| Case | s370_perf vers | median w50 |
| ---- | -------------- | :--------: |
| [2018-01-13_sys2-b](2018-01-13_sys2-b.md) | V0.9.1 rev  986 | 0.81% |
| [2018-03-04_sys2](2018-03-04_sys2.md)     | V0.9.5 rev  998 | 0.78% |
| [2018-03-31_sys2](2018-03-31_sys2.md)     | V0.9.7 rev 1003 | 0.89% |

The test reproducibility is very good in all three cases, the median of the
50% width is in the 0.80% to 0.90% range.

221 instruction times were determined with all three s370_perf versions.
In 209 cases the instruction times agree within 3%, in 182 cases even within 1%.

The 12 cases with a difference larger than 3% are
```
File num: name ----------------------------------- #test w50-med  w50-max
      01: 2018-03-31_sys2.dat                        330   0.89%     4.5%
      02: 2018-03-04_sys2.dat                        303   0.78%     3.8%
      03: 2018-01-13_sys2-b.dat                      233   0.81%     3.2%

Tag   Comment                :     tpi01     tpi02     tpi03 :  t02/t01  t03/t01
T252  TR m,m (10c)           :     16.44     29.28     27.85 :    1.781    1.694
T253  TR m,m (100c)          :     69.62    159.05    150.94 :    2.285    2.168
T254  TR m,m (250c)          :    151.23    361.12    347.36 :    2.388    2.297
T443  ZAP m,m (30d,10d)      :    100.39    100.25    104.03 :    0.999    1.036
T510  AER R,R                :     14.56     12.96     13.02 :    0.890    0.894
T511  AE R,m                 :     19.59     18.11     18.10 :    0.924    0.924
T522  AUR R,R                :     10.01     14.42     14.38 :    1.441    1.437
T540  ADR R,R                :     15.43     14.19     14.25 :    0.920    0.924
T541  AD R,m                 :     20.65     19.05     19.03 :    0.923    0.922
T560  AXR R,R                :     25.66     23.16     23.10 :    0.903    0.900
T600  STCK m                 :     65.48     97.36    102.95 :    1.487    1.572
T601  SPM R                  :      2.79      2.62      2.59 :    0.939    0.928
```

The tests T252 to T254 did not change, it is unclear why the times for `TR`
changed by about a factor 2.

#### <a id="find-itadd">T9xx tests and additivity of instruction times</a>
The [T9xx](../doc/s370_perf.md#user-content-tests-taux) tests allow to
check whether the instruction times are additive, or in other words,
whether the time for a sequence of instructions is the sum of the measured
instruction times.
For this analysis the [raw view](../data/2018-03-31_sys2-raw.dat) is 
suited best, see [-raw option](../doc/s370_perf_ana.md#user-content-opt-raw)
for format. It lists the time for a full inner loop, including the
closing `BCTR`.

The [T90x](../doc/s370_perf.md#user-content-tests-t90x) tests contain sequences
of `LR` instructions with increasing repeat count
```
T900  LR R,R (ig=1)   :       8.74
T901  LR R,R (ig=2)   :      10.82   dt =  2.08
T902  LR R,R (ig=3)   :      12.91   dt =  2.09
T903  LR R,R (ig=4)   :      30.27   dt = 17.36
T904  LR R,R (ig=5)   :      32.36   dt =  2.09
T905  LR R,R (ig=6)   :      34.40   dt =  2.04
T906  LR R,R (ig=7)   :      36.04   dt =  1.64
T907  LR R,R (ig=8)   :      38.32   dt =  2.28
T908  LR R,R (ig=9)   :      39.99   dt =  1.67
T909  LR R,R (ig=10)  :      42.58   dt =  2.59
```
The increment should be 2.09 ns, as determined by T100, which has a repeat
count of 100. One clearly see deviations, a particular drastic one at ig=4.
This is not a mistake in the test, the results on a P/390 show an very
different pattern, see
[p390 analysis](2018-02-14_p390.md#user-content-find-bctr).

The [T92x](../doc/s370_perf.md#user-content-tests-t92x) tests contain sequences
of `L` instructions with increasing repeat count
```
T920  L R,m (ig=1)    :      13.13
T921  L R,m (ig=2)    :      20.83   dt =  7.70
T922  L R,m (ig=3)    :      28.71   dt =  7.88
T923  L R,m (ig=4)    :      48.30   dt = 19.59
T924  L R,m (ig=5)    :      55.31   dt =  7.01
T925  L R,m (ig=6)    :      61.99   dt =  6.68
T926  L R,m (ig=7)    :      69.38   dt =  7.39
T927  L R,m (ig=8)    :      76.22   dt =  6.84
T928  L R,m (ig=9)    :      84.48   dt =  8.26
T929  L R,m (ig=10)   :      92.30   dt =  7.82
```
again significant deviations, strongest again at ig=4, but beyond that is the
pattern different.

The [T95x](../doc/s370_perf.md#user-content-tests-t95x) tests are chopped down
versions of the [T700](../doc/s370_perf.md#user-content-tests-t700) test.
Here the measured _total_ loop time should increase in each step by the
instruction time of the added instruction.
```
T952  T700 1st  2     :      11.38
T953  T700 1st  3     :      16.92
T954  T700 1st  4     :      21.42
T955  T700 1st  5     :      24.89
T956  T700 1st  6     :      29.34
T957  T700 1st  7     :      32.63
T958  T700 1st  8     :      40.59
T959  T700 1st  9     :      53.37
T960  T700 1st 10     :      55.34
T961  T700 1st 11     :      47.44  <-- drop
T962  T700 1st 12     :      52.72
...
T978  T700 1st 28     :     146.24
T979  T700 1st 29     :     138.52  <-- drop
T980  T700 1st 30     :     145.19
T981  T700 1st 31     :     139.25  <-- drop
T982  T700 1st 32     :     160.82
T983  T700 1st 33     :     153.04  <-- drop
T984  T700 1st 34     :     189.18
T985  T700 1st 35     :     161.15  <-- drop
T986  T700 1st 36     :     200.40
T987  T700 1st 37     :     182.43  <-- drop
T988  T700 1st 38     :     211.02
T989  T700 1st 39     :     202.47  <-- drop
T990  T700 1st 40     :     232.42
```
The loop time doesn't even increase monotonously, _adding_ an instruction
actually _decreases_ the total loop time in several cases.
Conclusion is sobering, instruction times are not strictly additive, so all
values deduced by s370_perf should be taken with a grain of salt.

#### <a id="find-lr">LR timing</a>
`LR` has the fastest instruction time, only 2.09 ns. Since the host CPU, a
[Xeon E5-1620](hostinfo_sys2.md), runs with a maximal clock of 3.6 GHz this
corresponds to only 7.5 clock cycles on the host CPU. Reasons behind
- `LR` has a short code path in Hercules, no condition code updates etc.
- a sequence of 100 `LR` instructions is tested, so the host CPU is well
  trained, [uop caches](https://en.wikipedia.org/wiki/CPU_cache#UOP-CACHE) or
  branch prediction has optimal hit rates.

#### <a id="find-ltr">LTR timing</a>
`LTR` is with 3.52 ns about 1.7 times slower than `LR`. Caused by the
additional computation for the condition code update, which in an emulator
simply adds to the instruction time. On a real CPU condition code handling
is a concurrent activity and doesn't add to the instruction time.

#### <a id="find-l">L+ST timing</a>
The simplest instructions with memory access are `L` and `ST` with 7.47 and
7.71 ns, about 3.7 more than a `LR` instruction, or about 27 clock cycles
on the host CPU. The substantial extra computation for the address translation
is clearly visible.

#### <a id="find-unal">Unaligned memory access</a>
s370_perf has several tests addressing unaligned memory access
([see docu](../doc/s370_perf.md#user-content-tests-unal)),
which can be summarized as
```
  T102  L R,m    :  7.47 ns | T103  L R,m (unal)    :  8.60 ns -> dt =  1.13 ns
  T110  ST R,m   :  7.71 ns | T111  ST R,m (unal)   :  8.65 ns -> dt =  0.94 ns

  T104  LH R,m   :  9.09 ns | T105  LH R,m (unal3)  :  9.48 ns -> dt =  0.39 ns
  T112  STH R,m  :  9.47 ns | T113  STH R,m (unal1) :  9.40 ns -> dt = -0.07 ns !
                            | T114  STH R,m (unal3) :  9.35 ns -> dt = -0.12 ns !

  T501  LE R,m   :  9.22 ns | T502  LE R,m (unal)   :  9.57 ns -> dt =  0.35 ns
  T508  STE R,m  :  9.14 ns | T509  STE R,m (unal)  :  9.71 ns -> dt =  0.57 ns
  T531  LD R,m   :  9.99 ns | T532  LD R,m (unal)   :  9.87 ns -> dt = -0.08 ns !
  T538  STD R,m  :  9.84 ns | T539  STD R,m (unal)  :  9.83 ns -> dt = -0.01 ns !
```
For `L` and `ST` the instruction time increases by about 1 ns, as one might
expect because an additional word must be read or written. The increment for
`LH` is with 0.39 ns much less, even though the misalignment of 3 bytes
requires the access of a 2nd word.

Astonishing is the behavior of `STH`, `LD` and `STD` where  unaligned access
offset is _slightly faster_ than the aligned access. This is not due to a
coding mistake in the tests. [Data from a P/390 system](2018-02-14_p390.md)
is consistent with expected behavior, `STH` with 3 byte misalignment and
the floating point cases exhibit 2 to 4 cycle additional delay
(see [2018-02-14_p390 data](../data/2018-02-14_p390.dat)).

#### <a id="find-bfar">Branch instruction timing</a>
s370_perf tests several branch instructions for two cases
- branch target in same page
- branch target in different page
  (see [far tests](../doc/s370_perf.md#user-content-tests-bc-far))

A branch to a different page is consistent substantially slower
```
                               near     far
  BNZ l (do br)          :     7.16   12.88
  BR R                   :     6.70   10.73
  BALR R,R; BR R         :    13.32   39.74
  BAL R,l; BR R          :    15.32   26.91
```
Apparently Hercules re-calculates the virtual to real address mapping
for an instruction fetch only when a new page is entered.

#### <a id="find-mfar">Memory access timing</a>
After detecting the same page vs different page differences for branch
instructions, see [branch timing](#user-content-find-bfar), it deemed
prudent to test also memory accesses speed
- in T701 data is on same page as the code
- in T702 data is on a different page as the code
beyond that both tests are identical. The measured _average_ instruction
time is the same
```
  T701  mix int RX           :    10.61
  T702  mix int RX (far)     :    10.62
```

#### <a id="find-alr">ALR timing</a>
The logical addition and subtraction, `ALR` and `SLR` is slightly faster
than the normal `AR` and `SR`
```
  T200  AR R,R  :   3.40 ns |  T203  ALR R,R  :   2.81 ns  -> dt = -0.59
  T205  SR R,R  :   3.86 ns |  T208  SLR R,R  :   2.87 ns  -> dt = -0.99
```
Caused by the additional computation for the exception checking, which in an
emulator simply adds to the instruction time. On a real CPU exception checking
is a concurrent activity and doesn't add to the instruction time.

#### <a id="find-float">Floating point performance</a>
The S/370 uses a
[hexadecimal floating point](https://en.wikipedia.org/wiki/IBM_Floating_Point_Architecture) format, so Hercules can't simply use the native
[IEEE floating point](https://en.wikipedia.org/wiki/IEEE_754) of the host
system. The instruction timing is, under these circumstances, very good
```
  single float            |  double float            |  integer
  T510  AER R,R :  14.56  |  T540  ADR R,R :  15.43  |  T200  AR R,R :   3.40
  T512  SER R,R :  14.64  |  T542  SDR R,R :  15.53  |  T205  SR R,R :   3.86
  T514  MER R,R :  10.06  |  T544  MDR R,R :  16.37  |  T210  MR R,R :   5.75
  T516  DER R,R :  20.30  |  T546  DDR R,R : 152.40  |  D215  DR R,R :  14.72
```
`DDR`, the double float division is slow, but that's a general feature of
floating division.

#### <a id="find-mvc">MVC performance</a>
The [T15x](../doc/s370_perf.md#user-content-tests-mvc) tests examine `MVC` over
a wide range of conditions
```
  Tag   Test                    tpi(ns)   byte/ns   dest buff comment
  T150  MVC m,m (5c)              17.66     0.28
  T151  MVC m,m (10c)             20.01     0.50
  T152  MVC m,m (15c)             19.15     0.78
  T153  MVC m,m (30c)             19.73     1.52
  T154  MVC m,m (100c)            23.10     4.32
  T155  MVC m,m (250c)            31.20     8.01
  T156  MVC m,m (250c,over1)     351.62     0.71     offset by + 1 byte
  T157  MVC m,m (250c,over2)      31.22     8.01     offset by -24 byte
```

`MVC` costs about 17 ns to setup, than copies data with about 18 GByte/sec.
Taken to be with a grain of salt because the test setup ensures that all
data is in L1 cache.

In T156 the destination buffer is offset by 1 byte relative to the source
buffer. This scheme is sometimes used to fill a buffer with a character.
This mode is a factor of ten slower that the other 250c cases, likely because
the underlying code has to handle this _byte-by-byte_.

#### <a id="find-mvcin">MVCIN performance</a>
The inverse copy `MVCIN` is naturally slower than `MVC`, which can use highly
optimizes copy routines, because it must be handled _byte-by-byte_.
`MVCIN` is much slower than the at least equally
complex `MVN` and `MVZ`, or `XC` which even does read-modify-write.
```
           MVC     MVN     MVZ   MVCIN       XC
   10c   20.01   20.06   20.02   73.19    22.35
   30c   19.73   33.78   33.75  204.04        -
  100c   23.10       -          667.17   101.14
```
It is not clear why `MVCIN` about a factor 6 slower that `MVN`, `MVZ` or `XC`.

#### <a id="find-mvcl">MVCL performance</a>
The [T17x](../doc/s370_perf.md#user-content-tests-mvcl) tests examine `MVCL`
over a wide range of conditions. The
[D17x lines](../data/2018-03-31_sys2.dat#L390-L399)
give the pure `MVCL` instruction times which can be compared with `MVC`
```
           MVC    MVCL
   10b   20.01   24.92
  100b   23.10   31.79
  250b   31.20   40.94
 1024b       -   96.68
 4096b       -  325.75
```
The setup time is higher, maybe because it is an interruptible instruction.
The copy throughput is for transfer sizes of 250 Bytes or below slightly
below `MVC`
the copy throughput comparable to `MVC`. On Hercules `MVCL` is about 35%
faster than a sequence of `MVC` instructions.

#### <a id="find-clc">CLC performance</a>
The [T27x](../doc/s370_perf.md#user-content-tests-clc) tests examine `CLC` over
a wide range of conditions. The `eq` tests compare two equal strings, so the
instruction must read all bytes of both buffers.
`CLC` is a factor 5.3 slower than `MVC`
```
Tag   Comment                      tpi
T276  CLC m,m (250c,eq)         165.90
T157  MVC m,m (250c,over2)       31.22
```
The `CLC` performance is comparable to other _byte-by-byte_ instructions,
like `XC`
```
T270  CLC m,m (10c,eq)  :    34.19  |  T235  XC m,m (10c)      :    22.35
T272  CLC m,m (30c,eq)  :    45.29
T274  CLC m,m (100c,eq) :    83.62  |  T236  XC m,m (100c)     :   101.14
T276  CLC m,m (250c,eq) :   165.90  |  T237  XC m,m (250c)     :   225.18
```

#### <a id="find-clcl">CLCL performance</a>
The [T28x](../doc/s370_perf.md#user-content-tests-clcl) tests examine `CLCL`
over a wide range of conditions. The
[D28x lines](../data/2018-03-31_sys2.dat#L402-L405)
give the pure `CLCL` instruction times which can be compared with `CLC`.
```
D281  CLCL (4kb,10b)    :   100.88  | T270  CLC m,m (10c,eq)  :    34.19
D282  CLCL (4kb,100b)   :   870.45  | T274  CLC m,m (100c,eq) :    83.62
D283  CLCL (4kb,250b)   :  2134.36  | T276  CLC m,m (250c,eq) :   165.90
```
`CLCL` is a factor 12.8 slower than `CLC` for a string compare over
250 matching characters, 2134.4 ns vs 165.9 ns. `CLCL` is the slowest
byte handling instruction, even slower than [TRT](#user-content-find-trt).

#### <a id="find-trt">TRT performance</a>
The [T25x](../doc/s370_perf.md#user-content-tests-trt) tests examine `TRT`
over a wide range of conditions. `TRT` is a factor 12 slower than `TR`
```
  T252  TR m,m (10c)    :    16.44 | T255  TRT m,m (10c,zero)  :    84.11
  T253  TR m,m (100c)   :    69.62 | T256  TRT m,m (100c,zero) :   782.46
  T254  TR m,m (250c)   :   151.23 | T257  TRT m,m (250c,zero) :  1936.51
```
In tests T255-T257 the lookup is always zero, so all input bytes are checked.
The complexity of `TR` and `TRT` seems quite similar, so the large
performance difference is astonishing.
[Data from a P/390 system](2018-02-14_p390.md) shows almost identical
instruction times for `TR` and `TRT` 
(see [2018-02-14_p390 data](../data/2018-02-14_p390.dat#L103-L110)).

#### <a id="find-dec">Decimal arithmetic performance</a>
The [T42x](../doc/s370_perf.md#user-content-tests-packed) tests examine
decimal packed arithmetic for 10 and 30 digit numbers.
The timing for
`AP`, `SP`, and `MP` is almost independent, only `DP` is significantly
faster for shorter numbers
```
              10d       30d
  AP m,m   221.84    217.21
  SP m,m   223.74    233.56
  MP m,m   277.65    277.56
  DP m,m   442.87   1030.89
```

#### <a id="find-lock">CS, CDS, TS performance</a>
Note: _the tests were performed with a dual CPU configuration_.

The [T29x](../doc/s370_perf.md#user-content-tests-cd) and
[T62x](../doc/s370_perf.md#user-content-tests-ts) tests examine the
`CS`, `CDS` and `TS` instructions, which use interlocked memory access
on a multi-CPU system.
They are typically used to implement
[locks](https://en.wikipedia.org/wiki/Lock_(computer_science)),
the available tests cover both cases
- lock _taken_: resource was free, is now locked
- lock _missed_: resource was busy
The timing is very different for _taken_ and _missed_
```
  lock taken                       |  lock missed
  -------------------------------- | --------------------------------
  LR;CS R,R,m (eq,eq)  :    38.89  |
  LR;CS R,R,m (eq,ne)  :    38.93  |  LR;CS R,R,m (ne)     :   176.75
  LR;CDS R,R,m (eq,eq) :    41.45  |
  LR;CDS R,R,m (eq,ne) :    41.49  |  LR;CDS R,R,m (ne)    :   178.58
  TS m (zero)          :    34.56  |  TS m (ones)          :   174.69
```
An inspection of the Hercules code showed that for the _lock missed_ case
```C
   if (sysblk.cpus > 1)  sched_yield();
```
is invoked. This forces a thread re-schedule on multi-CPU configurations,
which improves the performance of
[spinlocks](https://en.wikipedia.org/wiki/Spinlock).
[sched_yield()](http://man7.org/linux/man-pages/man2/sched_yield.2.html)
is a quite expensive system call, which results in the observed speed
difference.

#### <a id="find-stck">STCK performance</a>
Note: _the tests were performed without any Visualization layers_.

`STCK` returns the elapsed time, which is obtained by Hercules with the
[clock_gettime()](http://man7.org/linux/man-pages/man2/clock_gettime.2.html)
system call. On Linux this system call is handled via
[vDSO](https://en.wikipedia.org/wiki/VDSO) in user land without a context
switch to kernel mode. This results in a quite good instruction time of
65.48 ns.
