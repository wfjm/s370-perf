## Narrative: 2018-03-10_zpdt

**Objective**: Data from a z/PDT system

| Attribute | Value |
| --------- | ----- |
| Host   | _not disclosed_ |
| System | IBM z/PDT v1.7 with z/OS v2.2 |
| s370_perf | [V0.9.5  rev  998  2018-01-06](https://github.com/wfjm/s370-perf/blob/2d0b26d/codes/s370_perf.asm) |
| Creation Date | 2018-03-10 |
| Send by | _anonymous donor_ |
| Data | [2018-03-10_zpdt.dat](../data/2018-03-10_zpdt.dat) |

### Provisos <a name="prov"></a>
- the z/PDT license restricts the usage to: _"development, testing, employee education, or demonstration of applications that run on z/OS"_,
  and states explicitly that z/PDT must not be used for _"production workloads, robust development workloads, stress testing, or performance testing"_.
- this page is **not an attempt** to do any z/PDT **performance testing**
- in fact, [s370_perf](../doc/s370_perf.md) is **not the proper tool**
  to even try
- it merely gives **some observations on general features** of z/PDT

### Background <a name="back"></a>
The _System z Personal Development Tool_ or short z/PDT is marketed by IBM as
tool for _development, testing, employee education, or demonstration of applications that runs on z/OS_. In essence it emulates an IBM
[z series](https://en.wikipedia.org/wiki/IBM_Z) environment on a Linux system
hosted on an [Intel x86_64](https://en.wikipedia.org/wiki/X86-64) compatible CPU.
For more information see
[z/PDT redbook](http://www.redbooks.ibm.com/redbooks/pdfs/sg248205.pdf).

[Hercules](https://en.wikipedia.org/wiki/Hercules_(emulator)) is a plain
[interpreter](https://en.wikipedia.org/wiki/Interpreter_(computing)),
it processes each instruction in a strictly sequential fashion.
z/PDT goes a step further and employs
[binary translation](https://en.wikipedia.org/wiki/Binary_translation).
The z/PDT review
['What is this zPDT'](https://www.itconline.com/wp-content/uploads/2017/07/What-is-zPDT.pdf)
published by [ITC](https://www.itconline.com/)
in July 2017 describes this as
> More recent versions of zPDT have added a ["Just-In-Time" (JIT)](https://en.wikipedia.org/wiki/Just-in-time_compilation) compiled
> mode to this. Some algorithm determines whether a section of code should
> be interpreted or whether it would be better to invest some more initial
> cycles to compile the System z instructions into equivalent [x86 instructions](https://en.wikipedia.org/wiki/X86-64).

Given that a binary-to-binary compilation step is involved
**it is natural to assume** that
- an [_optimizing compiler_](https://en.wikipedia.org/wiki/Optimizing_compiler)
  is used
- the code is broken up into
  [basic blocks](https://en.wikipedia.org/wiki/Basic_block)
- a [data-flow analysis](https://en.wikipedia.org/wiki/Data-flow_analysis)
  is done, especially for register and condition code usage
- code is eliminated based on a [reaching definition](https://en.wikipedia.org/wiki/Reaching_definition) analysis

### Prior Art - prior taken literally <a name="priart"></a>
Binary translation is not a new approach, it has been used for example
very successfully more that 20 years ago by
[DEC](https://en.wikipedia.org/wiki/Digital_Equipment_Corporation)
in their [VAX](https://en.wikipedia.org/wiki/VAX) to
[Alpha](https://en.wikipedia.org/wiki/DEC_Alpha) migration strategy.
DEC provided a tool called VEST _(VAX Environment Software Translator)_
which _statically_ translated VAX to Aplha images. The technology is described
very well in
- [Binary translation, Comm of ACM 36(1993)69-81](https://dl.acm.org/citation.cfm?id=151227), also [doi:10.1145/151220.151227](https://doi.org/10.1145/151220.151227)
- [Binary translation, Digital Technical Journal Vol 4 No 4 (1992) page 137-152](http://www.bitsavers.org/pdf/dec/dtj/dtj_v04-04_1992.pdf) 
- [VAX to Alpha Migration guide, DEC  AA-QSBKB-TE, Nov 1996](https://support.hpe.com/hpsc/doc/public/display?docId=emr_na-c04623267)

Key difference is that VEST _statically_ translated one _application_ image,
running in one address space, so virtual to physical address mapping was done
by the target CPU. z/PDT handles a full system context, so has to handle
virtual to physical address translation at emulation level, for consequences
see the [section on RX instructions](#user-content-obs-rx).
The old papers cited above contain, despite this essential difference, lots of
background information on the technological aspects of binary translation.

### Available data <a name="data"></a>

The dataset provided by an _anonymous donor_ contains 11 s370_perf runs.
The parameters used to generated an analyze differ from the usual defaults
- [s370_perf](../doc/s370_perf.md) was used with
  [/G300](../doc/s370_perf.md#user-content-par-gnnn)
  because the default
  [/GAUT](../doc/s370_perf.md#user-content-par-gaut) gave erratic results.
- [s370_perf_ana](../doc/s370_perf_ana.md) was used with
  - [-dt=0](../doc/s370_perf_ana.md#user-content-opt-dt)
    to use the fastest run instead of the median value.
  - [-du=1](../doc/s370_perf_ana.md#user-content-opt-du)
    so that `w50` gives the _full_ width of the distribution.
  - [-nonrr](../doc/s370_perf_ana.md#user-content-opt-nonrr)
    so that the fields `n-rr` and `n-rx` were set to `0.00` because the
    analysis did not give any meaning results for these quantities.

The reason using for these settings will be obvious after consultation of
the [observations](#user-content-obs).

#### Host system <a name="host"></a>
The type of host system used for the z/PDT runs was not disclosed. Given
the cost of a z/PDT license, it is fair to assume that it had a performance
roughly equivalent to the _workstation type_ [sys2](hostinfo_sys2.md)
reference system.
The [lmark](README_narr.md#user-content-lmark) rating for
[sys2](2018-03-31_sys2.md) is 116 MIPS. Newer workstations will be
slightly higher, loaded high-end servers potentially lower. It is
reasonable to assume that the performance of the host system used for the
z/PDT tests deviates by not more than +- 25% from the `sys2` performance.
This +- 25% uncertainty is acceptable because later on only gross trends are
discussed.

### Observations <a name="obs"></a>
- z/PDT indeed does an optimizing compilation,
  see section [code optimization](#user-content-obs-opt).
- the same code is sometimes compiled, sometimes not,
  see section [to compile or not to compile](#user-content-obs-comp).
- if code is compiled, compilation can happen with substantial delay,
  see section [compilation delay](#user-content-obs-compdel).
- performance in plain interpretive mode seems similar to Hercules,
  see section [interpreter mode performance](#user-content-obs-inter).
- `RR` instructions is the easy part,
  see section [RR instructions](#user-content-obs-rr).
- `RX` instructions is the hard part,
  see section [RX instructions](#user-content-obs-rx).
- z/PDT vs Hercules comparisons difficult to interpret,
  see sections [z/PDT vs Hercules](#user-content-obs-sum).
- little gain for floating point arithmetic,
  see section [floating point performance](#user-content-obs-float).
- a bit faster for decimal packed arithmetic,
  see section [decimal packed performance](#user-content-obs-dec).
- `EX` is apparently always interpreted,
  see section [EX instruction](#user-content-obs-ex).
- z/PDT performs well for some instructions which are slow on Hercules
  - `CLCL` - see [CLCL performance](#user-content-obs-clcl)
  - `MVCIN` - see [MVCIN performance](#user-content-obs-mvcin)
  - `TRT` - see [TRT performance](#user-content-obs-trt)
- overall JIT gain depends heavily on workload,
  see [bottom line](#user-content-obs-bline).

#### Code Optimization <a name="obs-opt"></a>
The first striking feature of the results are the very small values for
the instruction time of simple instructions, some examples are
```
Tag   Comment                : nr     min     max      tpi   w50%
T100  LR R,R                 : 11    0.03    0.04   -0.000  38.5%
T101  LA R,n                 : 11    0.03    0.04    0.001  59.3%
T106  LTR R,R                : 11    0.04    0.06    0.011  59.5%
T107  LCR R,R                : 11    0.05    0.07    0.019  48.9%
T109  LPR R,R                : 11    0.05    0.06    0.020  30.4%
T108  LNR R,R                : 11    0.05    0.08    0.021  61.7%
T222  SRA R,1                : 11    0.11    0.16    0.022  45.5%
T224  SRA R,30               : 11    0.11    0.16    0.022  41.8%
```
Even native code can't possibly be that fast. The test loops simply repeat
the same instruction many times, with always the same target operand. The
compiler apparently detected this redundancy and optimized all but the first
instruction away. Since the analysis divides the measured time for the loop
by the number of _instructions under test_ in the loop, typically 50, one
gets a very small value.

#### to compile or not to compile <a name="obs-comp"></a>
Inspection of the results of the individual runs with the
[-ltpi](../doc/s370_perf_ana.md#user-content-opt-ltpi) option show that
for some tests the timing distribution is
[bimodal](https://en.wikipedia.org/wiki/Multimodal_distribution) like for
```
T700  mix int RR basic       : 11    0.17    9.30    0.103 5433.%    0.00   0.00
  tpi:     0.176     0.180     0.185     0.171     0.194
  tpi:     9.297     0.168     0.201     8.174     8.974
  tpi:     0.173
```
One gets either a value around `0.18` or a value around `9.00`.
The most natural explanation is that z/PDT in some runs decided to
compile, while in others it did not and stayed in interpreter mode.

This is also visible in the sometimes very large spread between the
`min` and the `max` time for tests, or equivalently, in the very large
`w50` values. Cases with a large spread can be easily selected with
```
  s370_perf_sort -k w50 2018-03-10_zpdt.dat
```

For tests with a large spread the _compile or interpret_ pattern was analyzed.
```
    c c c i c i i c i i c      4 tests
    c c c i c c c c c c c      4 tests
    c c c c c i i c i i c      5 tests
    c c c c c i c c i i c     53 tests
    c c c c c c c c c i c     30 tests
```
The 11 columns represent the 11 available z/PDT runs, a `c` indicates that
a test was apparently compiled (small `tpi` seen) and an `i` indicates that
a test was apparently interpreted (large `tpi` seen).
There is definitely no fixed pattern. The heuristics used by z/PDT
apparently involves factors beyond the instruction flow of the local
code section.

#### Compilation Delay <a name="obs-compdel"></a>
Some tests exhibit not only a bimodal distribution with a small and a large
`tpi` value, but also runs with values in-between, like for example
```
T503  LTER R,R               : 11    0.06    9.45    0.029 17087%    0.00   0.00
  tpi:     0.055     0.055     0.058     1.474     0.055
  tpi:     9.453     4.769     0.056     8.860     9.137
  tpi:     0.101
```
The simplest explanation is that for runs with a `tpi`
- around `0.055` z/PDT apparently compiled very early
- around `9.30` z/PDT apparently did not compile
- in-between (`1.4` and `4.7`) z/PDT compiles sometime during loop execution

#### Interpreter mode performance <a name="obs-inter"></a>
Under the assumption that the reasoning given in previous section
[compilation delay](#user-content-obs-compdel) is correct, it is plausible
to associate the largest `tpi` value seen in the bimodal distributions
with the _plain interpretative_ mode of z/PDT. Which can be directly
compared to Hercules results.

Under all these provisos one gets for the `LTER` instruction
- about 9.3 ns for the plain interpretative mode of z/PDT
- compared to 5.1 ns for Hercules on [sys2](2018-03-31_sys2.md),
  see [host proviso](#user-content-host).

Given the [JIT](https://en.wikipedia.org/wiki/Just-in-time_compilation)
nature of the z/PDT architecture, the z/PDT interpreter likely adds some
diagnostics to acquire information for the _compile-or-don't_ decision,
so it would not be astonishing if the plain z/PDT interpreter performance
is below that of Hercules.

#### RR instructions <a name="obs-rr"></a>
The test [T703](../doc/s370_perf.md#user-content-tests-t703) is designed
such that each calculated value is used. So z/PDT should _not_ be able to
optimize away some of the instructions. The result for this test, which is
a sequence of 38 `RR` and 2 `BC` instructions is
```
Tag   Comment                : nr     min     max      tpi   w50%
T703  mix int RR noopt       : 11    0.31    9.26    0.243 2906.%
```

The `tpi` reported for this test is the average time per instruction. A value
of 243 ps is not implausible and about the best one can expect if z/PDT does
a very careful data-flow analysis, eliminates all unnecessary condition code
calculations, dynamically maps S/370 registers to
[x86-64](https://en.wikipedia.org/wiki/X86-64)
registers etc (_all statements are pure speculation!_).

Obviously a large speed-up when compared to the 6.1 ns for the same test
with Hercules on [sys2](2018-03-31_sys2.md).

Bottom line: _RR instructions are the easy part for a JIT_.

#### RX instructions <a name="obs-rx"></a>
The tests [T701+T702](../doc/s370_perf.md#user-content-tests-t701) are a
sequence of 21 `RX` instructions. For T701 the data operands are in the
same page as the code, while for T702 the data operands are in a different
page. Like for T703 the average time per instruction is reported
```
Tag   Comment                : nr     min     max      tpi   w50%
T701  mix int RX             : 11    4.03    4.36    3.904   8.2%
T702  mix int RX (far)       : 11   15.82   16.99   15.696   7.4%
```

The `w50` values are very small, the code is always run in the same mode.
T702 is about a factor 4 slower than T701, so execution time apparently
_depends on the relative location of code and data_.

This highlights a major difficulty of the _system level_ emulation done by
z/PDT as well as Hercules. The
[virtual to physical address mapping](https://en.wikipedia.org/wiki/Virtual_memory)
must be calculated for each memory access, in IBM lingua the
[DAT box](https://en.wikipedia.org/wiki/Memory_management_unit#IBM_System/360_Model_67,_IBM_System/370,_and_successors) must be emulated. It is obvious that
this can easily be the most CPU time consuming part of a memory access, which
make it a perfect candidate for some heuristic optimizations.

In Hercules the instruction fetches have apparently some _DAT cache_, nicely
seen in the [branch timing behavior](2018-03-31_sys2.md#user-content-find-bfar)
where same page branches are faster than branches to a different page.
In contrast to this give the T701+T702 tests under Hercules always
essentially identical `tpi` values, see
[memory access timing behavior](2018-03-31_sys2.md#user-content-find-mfar).

The main conclusion is that z/PDT apparently  uses different heuristics
for the DAT box emulation.

Bottom line: _RX instructions are the hard part for a JIT doing a full system emulation_.

#### z/PDT vs Hercules comparisons <a name="obs-sum"></a>
Because z/PDT is a dynamic mixture of interpreter and compiler the concept of
an **instruction time is not well defined**. Technically it is straight forward
to produce instruction time comparisons with
[s370_perf_sum](../doc/s370_perf_sum.md) against all systems available in the
[data](../data) directory and described in the [narr](../narr) directory
with a command like
```
   s370_perf_sum -rat -i -k ins 2018-03-31_sys2.dat 2018-03-10_zpdt.dat
```
but the interpretation is difficult at best due to the impact of
- [code optimization](#user-content-obs-opt)
- [delayed compilation](#user-content-obs-compdel)

#### Floating point performance <a name="obs-float"></a>
The [S/370 floating point](https://en.wikipedia.org/wiki/IBM_Floating_Point_Architecture)
instructions can't be simply translated into
[x86_64](https://en.wikipedia.org/wiki/X86-64) instructions, z/PDT
must call library routines for all arithmetic operations, like Hercules does.
It is therefore prudent to compare the floating arithmetic instruction timings
with Hercules results. Using [sys2](2018-03-31_sys2.md) as reference system
(see [host proviso](#user-content-host)) gives
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T510  AER R,R                :     14.56     12.55 :    0.862
T514  MER R,R                :     10.06     10.71 :    1.065
T516  DER R,R                :     20.30     23.72 :    1.168
...
T540  ADR R,R                :     15.43     12.92 :    0.837
T544  MDR R,R                :     16.37     20.26 :    1.238
T546  DDR R,R                :    152.40     80.21 :    0.526
```

Apparently no essential speed difference, as one might expect. With the
exception of `DDR`, which is slowish for Hercules, z/PDT seems to have
a better implementation.

Side note: Because IBM mainframes support
[IEEE 754 floating point](https://en.wikipedia.org/wiki/IEEE_754) since
[S/390](https://en.wikipedia.org/wiki/IBM_System/390_ES/9000_Enterprise_Systems_Architecture_ESA_family) the old S/370 hex float is history and likely
not considered to be a performance critical part of z/PDT.

#### Decimal packed performance <a name="obs-dec"></a>
Decimal packed arithmetic is another obvious case where z/PDT can't simply
translate to
[x86_64](https://en.wikipedia.org/wiki/X86-64)
instructions and has to resort to library routines.
A comparison to
[sys2](2018-03-31_sys2.md)
, as always under [host proviso](#user-content-host), gives
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T420  AP m,m (10d)           :    221.84     29.93 :    0.135
T421  AP m,m (30d)           :    217.21     35.82 :    0.165
T424  MP m,m (10d)           :    277.65    125.20 :    0.451
T425  MP m,m (30d)           :    277.56    128.85 :    0.464
```

Apparently z/PDT seems to have the smarter library here.

#### EX instruction <a name="obs-ex"></a>
`EX` fetches another instruction, modified opcode, and executes the modified
instruction. This is obviously hard to compile into efficient native code,
and z/PDT apparently always simply interprets `EX`
(see [host proviso](#user-content-host))
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T610  EX R,i (with TM)       :     30.57    40.025 :    1.31
T611  EX R,i (with XI)       :     30.62    39.645 :    1.29
```

#### CLCL performance <a name="obs-clcl"></a>
On Hercules is `CLCL` quite slow when compared to `MVCL` and other byte
handling instructions,
see [sys2 CLCL performance](2018-03-31_sys2.md#user-content-find-clcl).
z/PDT seems to do much better
(see [host proviso](#user-content-host))
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
D280  CLCL (100b,10b)        :     98.28     35.20 :    0.358
D281  CLCL (4kb,10b)         :    100.88     34.59 :    0.343
D282  CLCL (4kb,100b)        :    870.45    122.61 :    0.141
D283  CLCL (4kb,250b)        :   2134.36    234.57 :    0.110
```

The `MVCL` performance is in contrast quite comparable
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
D170  MVCL (10b)             :     24.92     25.37 :    1.018
D171  MVCL (100b)            :     31.79     27.64 :    0.870
D172  MVCL (250b)            :     40.94     32.12 :    0.785
D173  MVCL (1kb)             :     96.68     54.12 :    0.560
D174  MVCL (4kb)             :    325.75    140.03 :    0.430
D175  MVCL (100b,pad)        :     19.89     36.35 :    1.828
D176  MVCL (1kb,pad)         :     36.18     67.88 :    1.876
D177  MVCL (4kb,pad)         :     97.64    151.57 :    1.552
D178  MVCL (1kb,over1)       :     10.77     12.30 :    1.142
D179  MVCL (1kb,over2)       :    103.63     93.26 :    0.900
```

#### MVCIN performance <a name="obs-mvcin"></a>
On Hercules is `MVCIN` quite slow compared to `MVN` and `MVZ`,
see [sys2 MVCIN performance](2018-03-31_sys2.md#user-content-find-mvcin).
z/PDT seems to do much better
(see [host proviso](#user-content-host))
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T167  MVCIN m,m (10c)        :     73.19     40.62 :    0.555
T168  MVCIN m,m (30c)        :    204.04     59.31 :    0.291
T169  MVCIN m,m (100c)       :    667.17    106.69 :    0.160
```

The `MVCIN` speed is much closer to `MVN` and `MVZ`
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T161  MVN m,m (10c)          :     20.06      8.59 :    0.428
T162  MVN m,m (30c)          :     33.78     24.88 :    0.736
T165  MVZ m,m (10c)          :     20.02      8.02 :    0.401
T166  MVZ m,m (30c)          :     33.75     23.18 :    0.687
```

#### TRT performance <a name="obs-trt"></a>
On Hercules is `TRT` quite slow compared to `TR`,
see [sys2 TRT performance](2018-03-31_sys2.md#user-content-find-trt).
Again, z/PDT seems to do significantly better
(see [host proviso](#user-content-host)) 
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T255  TRT m,m (10c,zero)     :     84.11     46.74 :    0.556
T256  TRT m,m (100c,zero)    :    782.46    240.78 :    0.308
T257  TRT m,m (250c,zero)    :   1936.51    501.74 :    0.259
T258  TRT m,m (250c,10b)     :     93.44     47.32 :    0.506
T259  TRT m,m (250c,100b)    :    799.32    246.97 :    0.309
```

with a _per byte_ performance much closer to `TR`
```
Tag   Comment                :      sys2      zPDT :  zPDT/sys2
T252  TR m,m (10c)           :     16.44     41.47 :    2.523
T253  TR m,m (100c)          :     69.62    116.19 :    1.669
T254  TR m,m (250c)          :    151.23    216.68 :    1.433
```

#### The bottom line <a name="obs-bline"></a>
The overall JIT gain depends heavily on the instruction mix. Code
dominated by longer stretches of RR instructions and other instructions
without data memory access will give a large gain, while code dominated
by RX instructions or complex instructions like decimal arithmetic will
give a much more modest gain. The only practical example available in this
context is the CPU time of the assembly step of the s370_perf jobs.
The assembly time clearly shows an improvement compared to the best
performing Hercules systems studied so far, but the factor is quite modest.
